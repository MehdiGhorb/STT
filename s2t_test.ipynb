{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myenviron (Python 3.10.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n myenviron ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from torchaudio.transforms import TimeMasking, FrequencyMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "NUM_CLASSES = 28  # 26 letters + space + blank for CTC\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Character Mapping\n",
    "char_map = {c: i for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}\n",
    "char_map[''] = 27  # CTC blank token\n",
    "idx_map = {i: c for c, i in char_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing with Augmentation\n",
    "class AudioProcessor:\n",
    "    def __init__(self):\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n",
    "        self.resample = Resample(orig_freq=48000, new_freq=SAMPLE_RATE)\n",
    "        self.time_mask = TimeMasking(time_mask_param=80)\n",
    "        self.freq_mask = FrequencyMasking(freq_mask_param=30)\n",
    "\n",
    "    def __call__(self, waveform, sample_rate):\n",
    "        if sample_rate != SAMPLE_RATE:\n",
    "            waveform = self.resample(waveform)\n",
    "        mel_spec = self.mel_spec(waveform).squeeze(0)\n",
    "        mel_spec = self.time_mask(mel_spec)  # Apply time mask\n",
    "        mel_spec = self.freq_mask(mel_spec)  # Apply frequency mask\n",
    "        return mel_spec\n",
    "\n",
    "def text_to_int(text):\n",
    "    return torch.tensor([char_map[c] for c in text.lower() if c in char_map], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels, input_lengths, label_lengths = [], [], [], []\n",
    "    for waveform, _, text, _, _, _ in batch:\n",
    "        spec = processor(waveform, SAMPLE_RATE)\n",
    "        waveforms.append(spec.T)\n",
    "        labels.append(text_to_int(text))\n",
    "        input_lengths.append(spec.shape[1])  # Time dimension\n",
    "        label_lengths.append(len(labels[-1]))\n",
    "    \n",
    "    waveforms = pad_sequence(waveforms, batch_first=True).permute(0, 2, 1)\n",
    "    labels = pad_sequence(labels, batch_first=True)\n",
    "    return waveforms, labels, torch.tensor(input_lengths), torch.tensor(label_lengths)\n",
    "\n",
    "def int_to_text(int_seq):\n",
    "    return ''.join([idx_map[i] for i in int_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition with BatchNorm and Dropout\n",
    "class STTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STTModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=N_MELS, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE * 2, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [batch, 1, freq, time]\n",
    "        x = self.conv(x)  # [batch, channels, freq, time]\n",
    "        x = x.permute(0, 3, 2, 1)  # [batch, time, freq, channels]\n",
    "        x = x.mean(dim=-1)  # Reduce channels: [batch, time, freq]\n",
    "        x, _ = self.rnn(x)  # Input now matches N_MELS\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "model = STTModel().to(DEVICE)\n",
    "ctc_loss = nn.CTCLoss(blank=27)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "# Early Stopping Setup\n",
    "best_loss = np.inf\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Function\n",
    "def evaluate(validation_loader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels, input_lengths, label_lengths in validation_loader:\n",
    "            waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(waveforms)\n",
    "            log_probs = nn.functional.log_softmax(outputs, dim=2)\n",
    "            val_loss = ctc_loss(log_probs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "def train(train_loader, validation_loader):\n",
    "    global best_loss, patience_counter\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for waveforms, labels, input_lengths, label_lengths in train_loader:\n",
    "            waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(waveforms)\n",
    "            log_probs = nn.functional.log_softmax(outputs, dim=2)\n",
    "            loss = ctc_loss(log_probs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), f\"model_{epoch+1}.pth\")\n",
    "\n",
    "        # Validation Loss\n",
    "        val_loss = evaluate(validation_loader)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "        model.train()\n",
    "\n",
    "        # Learning Rate Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and Initialize Processor\n",
    "processor = AudioProcessor()\n",
    "train_dataset = LIBRISPEECH(\"./data\", url=\"train-clean-360\", download=True)\n",
    "validation_dataset = LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Run Training\n",
    "train(train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenviron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
